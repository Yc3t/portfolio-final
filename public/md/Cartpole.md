Lets undestand the cartpole example in reinforcement learning, using tinygrad library
![[Cartpole-20240308194945810.png|500]]

We'll use tensor.py,TinyJit and nn

Basically what this does is reinforcement learning until 100%, until the time is off

We got this hyperparameters:

BATCH_SIZE -> size of the sameples used in each training step. A big batch size could lead to more stable gradients, but will require more memory and computation

ENTROPY_SCALE -> 

REPLAY_BUFFER_SIZE ->

PRO_EPSILON ->

HIDDEN_UNITS ->

LEARNING_RATE ->

TRAIN_STEPS ->

EPISODES ->

DISCOUNT_FACTOR ->




